{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_fa8JdgGjiE"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/PBL/flooding/dataset/flood_dataset5.zip -d ../content/flood_dataset5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 처리와 시각 처리를 위한 도구\n",
        "import numpy as np # 다차원 배열과 행렬 연산\n",
        "import os # 운영체제\n",
        "import cv2 # OpenCV 라이브러리\n",
        "import matplotlib.pyplot as plt # Matplotlib 라이브러리, 데이터 시각화 라이브러리, 그래프 및 도표 생성\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "V1ObKMihGmtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flood_images_path = r\"../content/flood_dataset5/flood4\"\n",
        "non_flood_images_path = r\"../content/flood_dataset5/non_flood4\""
      ],
      "metadata": {
        "id": "uzaQk-YyGoD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 침수 이미지 표시\n",
        "for img in os.listdir(flood_images_path):# 화재 데이터셋 가져오기\n",
        "    img_array = cv2.imread(os.path.join(flood_images_path,img)) # 화재 이미지 opencv로 읽어서 img_array에 저장\n",
        "    plt.imshow(img_array) # img_array에 저장된 이미지 표시\n",
        "    plt.title(img)  # 이미지 위에 파일 이름을 출력\n",
        "    plt.show() # 화면에 이미지 출력\n",
        "    break # 맨 처음 이미지 출력하고 종료, 이거 제거하면 여러 이미지 출력 가능"
      ],
      "metadata": {
        "id": "9BqeafFMGpt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 비침수 이미지 표시\n",
        "for img in os.listdir(non_flood_images_path):\n",
        "    img_array = cv2.imread(os.path.join(non_flood_images_path,img))\n",
        "    plt.imshow(img_array)\n",
        "    plt.show()\n",
        "    break"
      ],
      "metadata": {
        "id": "wPFJCrQ8Gq-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# img_array의 형태 표시, 이미지 크기와 색상 채널 수 표시\n",
        "img_array.shape # (높이, 너비, 채널) 튜플 반환, 채널이 3이면 컬러, 1이면 흑백"
      ],
      "metadata": {
        "id": "MI4n43zAGsst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 데이터 전처리\n",
        "data = [] # 이미지 데이터와 클래스 레이블을 저장할 리스트 선언\n",
        "IMG_SIZE = 32 # 모든 이미지 32x32 픽셀로 조정\n",
        "\n",
        "def get_data(file_path,class_num, data): # 이미지 데이터 data에 저장\n",
        "    for img in os.listdir(file_path):\n",
        "        try:\n",
        "            img_array = cv2.imread(os.path.join(file_path,img)) # 이미지 파일 경로 저장, 파일을 img_array에 저장\n",
        "            new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE)) # 이미지 크기 리사이징\n",
        "            data.append([new_array, class_num]) # 이미지와 클래스 레이블 엮서서 data 리스트에 추가\n",
        "        except:\n",
        "            pass # 예외 무시하고 처리\n",
        "get_data(flood_images_path,1,data) # 1이면 침수 이미지\n",
        "get_data(non_flood_images_path,0,data) # 0이면 비침수 이미지"
      ],
      "metadata": {
        "id": "zLbvV53cGt3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data 리스트의 개수 반환\n",
        "len(data)"
      ],
      "metadata": {
        "id": "b5wrWL00GvPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 텐서플로우 모델 정의\n",
        "import tensorflow as tf # 텐서플로우 라이브러리 가져오기\n",
        "from tensorflow import keras # 텐서플로우의 케라스 모듈 가져오기\n",
        "from tensorflow.keras import layers, models # 케라스에서 서브 모듈인 layers, models 가져오기\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout # 순서대로 연결레이어, 활성화함수, 배열평탄화, 과적합방지"
      ],
      "metadata": {
        "id": "hxrmVh9RGwha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 증강으로 이미지 데이터셋 확장 (텐서플로우 이용)\n",
        "data_augmentation1 = tf.keras.Sequential([  # 데이터 증강 파이프라인 정의 1\n",
        "  layers.experimental.preprocessing.RandomZoom(0.5), # 이미지의 0~50% 범위에서 무작위로 줌 적용\n",
        "])\n",
        "\n",
        "data_augmentation2 = tf.keras.Sequential([ # 데이터 증강 파이프라인 정의 2\n",
        "  layers.experimental.preprocessing.RandomContrast(0.2) # 이미지의 대비를 무작위로 조정\n",
        "])\n",
        "\n",
        "def apply_augmentation(data): # 데이터 증강 함수\n",
        "    flood_count = sum(1 for _, label in data if label == 1) # 침수 이미지의 개수 계산\n",
        "    non_flood_count = sum(1 for _, label in data if label == 0) # 비침수 이미지의 개수 계산\n",
        "\n",
        "    while non_flood_count < flood_count: # 비침수 이미지 수가 침수 이미지 수와 같아질 때까지 반복\n",
        "        for i in range(len(data)):\n",
        "            if data[i][1] == 0: # 데이터 레이블이 0, 즉 비침수 이미지에만 데이터 증강 수행\n",
        "                data.append([data_augmentation1(data[i][0]), 0]) # 변형된 추가 데이터 생성\n",
        "                non_flood_count += 1 # 비침수 이미지 개수 증가\n",
        "                if non_flood_count >= flood_count: # 비침수 이미지 수가 침수 이미지 수와 같거나 크면 반복 종료\n",
        "                    break\n",
        "\n",
        "apply_augmentation(data) # 변형해서 확장된 데이터셋을 원래 데이터와 함께 data 리스트에 추가\n",
        "\n",
        "# 침수 이미지 수와 비침수 이미지 수 출력\n",
        "flood_count = sum(1 for _, label in data if label == 1)\n",
        "non_flood_count = sum(1 for _, label in data if label == 0)\n",
        "print(\"침수 이미지 수:\", flood_count)\n",
        "print(\"비침수 이미지 수:\", non_flood_count)\n",
        "\n",
        "len(data) # 전체 데이터셋 크기 확인"
      ],
      "metadata": {
        "id": "_t4RJQKcGyOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 증강으로 이미지 데이터셋 확장 (텐서플로우 이용)\n",
        "data_augmentation1 = tf.keras.Sequential([  # 데이터 증강 파이프라인 정의 1\n",
        "  layers.experimental.preprocessing.RandomZoom(0.5), # 이미지의 0~50% 범위에서 무작위로 줌 적용\n",
        "])\n",
        "\n",
        "data_augmentation2 = tf.keras.Sequential([ # 데이터 증강 파이프라인 정의 2\n",
        "  layers.experimental.preprocessing.RandomContrast(0.2) # 이미지의 대비를 무작위로 조정\n",
        "])\n",
        "\n",
        "def apply_augmentation(data): # 데이터 증강 함수\n",
        "    for i in range(len(data)):\n",
        "        if data[i][1] == 0 or data[i][1] == 1:\n",
        "            data.append([data_augmentation1(data[i][0]), data[i][1]])  # 변형된 추가 데이터 생성\n",
        "            data.append([data_augmentation2(data[i][0]), data[i][1]])  # 변형된 추가 데이터 생성\n",
        "apply_augmentation(data) # 변형해서 확장된 데이터셋을 원래 데이터와 함께 data 리스트에 추가\n",
        "len(data) # 전체 데이터셋 크기 확인"
      ],
      "metadata": {
        "id": "-hJ3XxCxG0Cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 섞기\n",
        "import random\n",
        "random.shuffle(data) # 데이터셋 랜덤으로 섞기 (모델 학습 시에 데이터 순서에 영향 안 받게 함)"
      ],
      "metadata": {
        "id": "y76Rq02TG2p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 데이터와 클래스 레이블 분리 (데이터셋 준비 과정)\n",
        "X = [] # 이미지 데이터 저장 리스트\n",
        "y = [] # 이미지의 클래스 레이블 저장 리스트\n",
        "\n",
        "for i in data:\n",
        "    X.append(i[0]) # i[0]이 데이터의 첫 번째 요소로 이미지 데이터\n",
        "    y.append(i[1]) # i[1]이 데이터의 두 번재 요소로 클래스 레이블"
      ],
      "metadata": {
        "id": "jnr0DbO-G35j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋에서 이미지, 클래스 레이블 확인\n",
        "X[0],y[0]"
      ],
      "metadata": {
        "id": "M9RCnodPG3_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 클래스 레이블 데이터 시각화\n",
        "plt.hist(y)"
      ],
      "metadata": {
        "id": "sneUxhlSG6w6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 데이터 형태 조정, X랑 y 리스트 각각 배열 변환\n",
        "X = np.array(X).reshape(-1,IMG_SIZE, IMG_SIZE,3) # 4차원 배열로 변환\n",
        "y = np.array(y) # 1차원 배열로 변환"
      ],
      "metadata": {
        "id": "ZtFwT9g5G7_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 배열 X와 y 형태 변환\n",
        "X.shape, y.shape\n",
        "# X는 (데이터 개수, 높이, 너비, 채널 수), y는 (클래스 레이블 개수, )"
      ],
      "metadata": {
        "id": "iSuiTLbBG9IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋을 학습 용과 테스트 용으로 나누기\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 데이터 X와 레이블 y를 학습 셋과 테스트 셋으로 나눔, 테스트 셋은 20%\n",
        "# 테스트 셋을 검증 셋과 최종 테스트 셋으로 나눔\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2)\n",
        "X_test,X_val,y_test,y_val = train_test_split(X_test,y_test,test_size=0.5)"
      ],
      "metadata": {
        "id": "ehGTZi7MG-Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 형태 출력\n",
        "print(X_train.shape) # 학습 세트의 형태 출력 (데이터 수, 이미지 높이, 너비, 채널 수)\n",
        "print(X_test.shape) # 테스트 세트의 형태 출력\n",
        "print(X_val.shape) # 검증 세트의 형태 출력"
      ],
      "metadata": {
        "id": "VocV-HenHBH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 텐서플로우로 모델 정의\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout"
      ],
      "metadata": {
        "id": "j2ki8bMiHC8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 분류\n",
        "model = models.Sequential([ # 순차적으로 레이어를 쌓아가는 Keras의 모델\n",
        "  # 첫 번째 합성곱 레이어 (Conv2D), 입력 이미지의 특징 추출\n",
        "  # 'padding'은 'same'으로 설정되어 입력과 출력 크기 동일하게 유지\n",
        "  layers.Conv2D(32, kernel_size = (3,3), padding='same', activation='relu', input_shape=(IMG_SIZE,IMG_SIZE,3)),\n",
        "  # 첫 번째 맥스 풀링 레이어 (MaxPooling2D)\n",
        "  # 공간 크기 감소 및 계산 비용을 위해 사용\n",
        "  layers.MaxPooling2D(),\n",
        "\n",
        "  layers.Conv2D(64, kernel_size = (3,3), padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "\n",
        "  layers.Dropout(0.5), # 드롭 아웃 레이어\n",
        "\n",
        "  layers.Conv2D(64, kernel_size = (3,3), padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "\n",
        "  layers.Conv2D(64, kernel_size = (3,3), padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "\n",
        "  # 최종 분류 수행\n",
        "  layers.Flatten(), # 다차원 배열을 1차원 배열로 변환\n",
        "  layers.Dense(60, activation='relu'),  # 첫 번째 Fully Connected 레이어 (Dense), 모든 뉴런이 이전 레이어의 모든 뉴런과 연결\n",
        "  layers.Dropout(0.5), # 드롭 아웃 레이어\n",
        "  layers.Dense(1, activation='sigmoid') # 두 번째 Fully Connected 레이어 (Dense), 이진 분류 문제에서 출력을 생성\n",
        "])"
      ],
      "metadata": {
        "id": "97OLAYSLHEAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 구조 출력\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "VE7-7zbdHFLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 컴파일\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "custom_optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(loss='binary_crossentropy', optimizer=custom_optimizer, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "uedc-yD4HGit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 훈련, epochs는 반복하는 학습 횟수 에폭\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=40) # train은 훈련 데이터, validation은 검증 데이터"
      ],
      "metadata": {
        "id": "QThlvycZHH82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 정확도 함수\n",
        "plt.figure(0) # 그래프 창 생성\n",
        "plt.plot(history.history['accuracy'], label='training accuracy') # 학습 정확도 라벨 붙여서 표시\n",
        "plt.plot(history.history['val_accuracy'], label='val accuracy') # 검증 정확도 라벨 붙여서 표시\n",
        "plt.title(\"Accuracy\") # 그래프 제목 설정\n",
        "plt.xlabel('epochs') # x축 레이블을 학습 수로 설정\n",
        "plt.ylabel('accuracy') # y축 레이블을 정확도로 설정\n",
        "plt.legend() # 그래프에 범례 추가\n",
        "plt.show() # 화면에 그래프 표시\n",
        "\n",
        "# 손실 함수 (오차 함수)\n",
        "plt.figure(1) # 그래프 창 생성\n",
        "plt.plot(history.history['loss'], label='training loss') # 손실\n",
        "plt.plot(history.history['val_loss'], label='val loss') # 검증 손실\n",
        "plt.title('Loss') # 그래프 제목 설정\n",
        "plt.xlabel('epochs') # x축 레이블을 학습 수로 설정\n",
        "plt.ylabel('loss') # y축 레이블을 손실로 설정\n",
        "plt.legend() # 그래프에 범례 추가\n",
        "plt.show() # 화면에 그래프 표시"
      ],
      "metadata": {
        "id": "kaXiYwWkHKsW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}